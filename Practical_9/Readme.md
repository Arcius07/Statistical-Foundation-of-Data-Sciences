üå∏ Iris Dataset ‚Äî KNN Classification Analysis
üß≠ Objective

The goal of this practical is to apply the K-Nearest Neighbors (KNN) algorithm on the Iris dataset to classify flower species.
This experiment includes:

Exploratory Data Analysis (EDA)

Feature Scaling

Model Training

Error Rate Comparison

Selecting the Best K

Model Evaluation & Visualization

This practical helps understand how distance-based classification works in machine learning.

üìä Dataset Description

The Iris dataset contains measurements of 150 iris flowers belonging to three different species.

üìÅ Columns Description
Column	Description
sepal length (cm)	Length of the sepal
sepal width (cm)	Width of the sepal
petal length (cm)	Length of the petal
petal width (cm)	Width of the petal
species	Target variable: Setosa, Versicolor, Virginica
üß∞ Tools & Libraries Used
Tool / Library	Purpose
Python (3.x)	Programming environment
pandas	Data loading & EDA
NumPy	Numerical operations
scikit-learn	KNN model, scaling, metrics
Matplotlib	Graph plotting
Jupyter Notebook / Google Colab	Code execution
üß™ Procedures & Results
1Ô∏è‚É£ Exploratory Data Analysis (EDA)

Performed using:

head() ‚Üí Preview dataset

describe() ‚Üí Summary statistics

groupby() ‚Üí Group statistics by species

This helps to understand feature distribution and species differences.

2Ô∏è‚É£ Feature Scaling

Used StandardScaler to normalize numerical features.
Scaling is crucial because KNN relies on distance measurements.

3Ô∏è‚É£ Train-Test Split & KNN Model Training

Dataset split ‚Üí 70% training, 30% testing

Initial model trained with K = 5

4Ô∏è‚É£ Confusion Matrix & Accuracy Score

After prediction:

Confusion matrix shows classification performance

Accuracy score confirms high model performance (typically >95%)

5Ô∏è‚É£ Classification Report

Includes:

Precision

Recall

F1-score

All three species achieve high performance due to Iris‚Äô clean separation.

6Ô∏è‚É£ Error Rate Comparison for K = 1 to 20

K values from 1 to 20 were tested to observe how error changes.
This helps determine the most stable and accurate K value.

7Ô∏è‚É£ Plot: Error Rate vs K Values

A line plot was created to visualize:

Error patterns

Which K values perform best

Overfitting at lower K values

Stability at higher K values

8Ô∏è‚É£ Finding the Best K

Best K = value with minimum error rate
Usually found around K = 3‚Äì7 for the Iris dataset.

9Ô∏è‚É£ Visualization of Results (PCA)

Dimensionality reduction using PCA (2 components) was performed to plot:

Points colored by species

Clear separation between Setosa, Versicolor, Virginica

üìà Summary of Findings
Step	Purpose	Result
EDA	Understand dataset	Clear differences between species
Scaling	Normalize features	Essential for KNN
Model Training	Fit KNN	Initial K=5 performed well
Confusion Matrix	Evaluate predictions	Very high accuracy
Error Rate Analysis	Find optimal K	Best K around 3‚Äì7
PCA Visualization	Show class clusters	Species visually separable
üèÅ Conclusion

This practical provided hands-on experience in:

‚úî Performing KNN classification
‚úî Understanding the need for feature scaling
‚úî Evaluating model performance
‚úî Testing multiple K values to find the optimal one
‚úî Visualizing high-dimensional data using PCA

Key Insights

KNN performs extremely well on the Iris dataset due to its natural class separation.

Choosing an optimal K is important for reducing error and improving accuracy.

Visualization techniques like PCA help understand dataset structure.
